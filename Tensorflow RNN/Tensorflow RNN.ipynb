{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "65cd9972",
      "metadata": {
        "id": "65cd9972"
      },
      "source": [
        "# Final Assignment\n",
        "\n",
        "- Finish shakespeare (but add more training examples where you're not just predicting the end of a sentence, but also next words) and produce a nice sounding sonnet. We'll read each others' sonnets in class.\n",
        "\n",
        "- Do a corpus in your own language\n",
        "\n",
        "- Finish training addition with the tensorflow RNN (one-hot encoded)\n",
        "\n",
        "- Train a *dense* network to add two numbers\n",
        "\n",
        "- Start working on your final project: An RNN/CNN to predict LES snow.\n",
        "\n",
        "- What's the difference between **dense** networks and **recurrent** networks, in your words? Please be able to answer this question!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38df4cbe",
      "metadata": {
        "id": "38df4cbe"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import itertools\n",
        "import operator\n",
        "import numpy as np\n",
        "import nltk\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "1WUuvoLc78jI"
      },
      "id": "1WUuvoLc78jI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f75867fb",
      "metadata": {
        "id": "f75867fb"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk import tokenize\n",
        "\n",
        "#alphabets= \"([A-Za-z])\"\n",
        "#prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
        "#suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
        "#starters = \"(Mr|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
        "#acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
        "#websites = \"[.](com|net|org|io|gov|edu|me)\"\n",
        "#digits = \"([0-9])\"\n",
        "\n",
        "# vocabulary_size = 8000\n",
        "vocabulary_size = 3000\n",
        "\n",
        "unknown_token = \"UNKNOWN_TOKEN\"\n",
        "sentence_start_token = \"SENTENCE_START\"\n",
        "sentence_end_token = \"SENTENCE_END\"\n",
        "\n",
        "def clean_roman_numerals(text):\n",
        "    pattern = r\"\\b(?=[MDCLXVIΙ])M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})([IΙ]X|[IΙ]V|V?[IΙ]{0,3})\\b\\.?\"\n",
        "    return re.sub(pattern, '&', text)\n",
        "\n",
        "# Read the data and append SENTENCE_START and SENTENCE_END tokens\n",
        "print( \"Reading txt file...\")\n",
        "with open(r'/content/shakespeare-sonnets.txt', 'r', encoding=\"ISO-8859-1\") as f:\n",
        "    text = f.read()\n",
        "    \n",
        "    text = text.replace(\",\",\".\")\n",
        "    text = text.replace(\":\",\".\")\n",
        "    text = text.replace(\";\",\".\")\n",
        "    text = text.replace(\"?\",\".\")\n",
        "    text = text.replace(\"!\",\".\")\n",
        "    \n",
        "    text = clean_roman_numerals(text)\n",
        "    \n",
        "    #text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
        "    #text = re.sub(websites,\"<prd>\\\\1\",text)\n",
        "    #text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
        "    #if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
        "    #if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
        "    #text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
        "    #text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
        "    #text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
        "    #text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
        "    #text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
        "    #text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
        "    #text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
        "    #if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
        "    #if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
        "    #if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
        "    #if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
        "    #text = text.replace(\".\",\".<stop>\")\n",
        "    #text = text.replace(\"?\",\"?<stop>\")\n",
        "    #text = text.replace(\"!\",\"!<stop>\")\n",
        "    #text = text.replace(\"<prd>\",\".\")\n",
        "    #sentences = text.split(\"<stop>\")\n",
        "    #sentences = sentences[:-1]\n",
        "    #sentences = [s.strip() for s in sentences]\n",
        "    \n",
        "    sentences = tokenize.sent_tokenize(text)\n",
        "    \n",
        "    # Append SENTENCE_START and SENTENCE_END\n",
        "    sentences = [\"%s %s %s\" % (sentence_start_token, x, sentence_end_token) for x in sentences]\n",
        "    \n",
        "print(  \"Parsed %d sentences.\" % (len(sentences)))\n",
        "    \n",
        "# Tokenize the sentences into words\n",
        "tokenized_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
        "\n",
        "# Count the word frequencies\n",
        "word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
        "print(  \"Found %d unique words tokens.\" % len(word_freq.items()))\n",
        "\n",
        "# Get the most common words and build index_to_word and word_to_index vectors\n",
        "vocab = word_freq.most_common(vocabulary_size-1)\n",
        "index_to_word = [x[0] for x in vocab]\n",
        "index_to_word.append(unknown_token)\n",
        "word_to_index = dict([(w,i) for i,w in enumerate(index_to_word)])\n",
        "\n",
        "print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
        "print(\"The least frequent word in our vocabulary is '%s' and appeared %d times.\" % (vocab[-1][0], vocab[-1][1]))\n",
        "\n",
        "# Replace all words not in our vocabulary with the unknown token\n",
        "for i, sent in enumerate(tokenized_sentences):\n",
        "    tokenized_sentences[i] = [w if w in word_to_index else unknown_token for w in sent]\n",
        "\n",
        "print(  \"\\nExample sentence: '%s'\" % sentences[0])\n",
        "print(  \"\\nExample sentence after Pre-processing: '%s'\" % tokenized_sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bec2349b",
      "metadata": {
        "id": "bec2349b"
      },
      "outputs": [],
      "source": [
        "X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
        "y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2e3a12",
      "metadata": {
        "id": "5a2e3a12"
      },
      "outputs": [],
      "source": [
        "X_train.shape, y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7eb5e90a",
      "metadata": {
        "id": "7eb5e90a"
      },
      "outputs": [],
      "source": [
        "class RNN:    \n",
        "    def __init__(self, word_dim, hidden_dim=100, bptt_truncate=4):\n",
        "        # Assign instance variables\n",
        "        self.word_dim = word_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.bptt_truncate = bptt_truncate\n",
        "        \n",
        "        # Randomly initialize the network parameters\n",
        "        self.U = np.random.uniform(-np.sqrt(1./word_dim), np.sqrt(1./word_dim), (hidden_dim, word_dim))\n",
        "        self.V = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (word_dim, hidden_dim))\n",
        "        self.W = np.random.uniform(-np.sqrt(1./hidden_dim), np.sqrt(1./hidden_dim), (hidden_dim, hidden_dim))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7730ae",
      "metadata": {
        "id": "1e7730ae"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ca4f76",
      "metadata": {
        "id": "31ca4f76"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(self, x):\n",
        "    # The total number of time steps\n",
        "    T = len(x)\n",
        "    \n",
        "    # During forward propagation we save all hidden states in s because need them later.\n",
        "    # We add one additional element for the initial hidden, which we set to 0\n",
        "    s = np.zeros((T + 1, self.hidden_dim))\n",
        "    s[-1] = np.zeros(self.hidden_dim)\n",
        "    \n",
        "    # The outputs at each time step. Again, we save them for later.\n",
        "    o = np.zeros((T, self.word_dim))\n",
        "    \n",
        "    # For each time step...\n",
        "    for t in np.arange(T):\n",
        "        # Note that we are indxing U by x[t]. This is the same as multiplying U with a one-hot vector.\n",
        "        s[t] = np.tanh(self.U[:,x[t]] + self.W.dot(s[t-1]))\n",
        "        o[t] = softmax(self.V.dot(s[t]))\n",
        "        \n",
        "    return [o, s]\n",
        "\n",
        "RNN.forward_propagation = forward_propagation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0b0674b",
      "metadata": {
        "id": "d0b0674b"
      },
      "source": [
        "We not only return the calculated outputs, but also the hidden states. We will use them later to calculate the gradients, and by returning them here we avoid duplicate computation. Each `o`  is a vector of probabilities representing the words in our vocabulary, but sometimes, for example when evaluating our model, all we want is the next word with the highest probability. We call this function `predict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479a7bfe",
      "metadata": {
        "id": "479a7bfe"
      },
      "outputs": [],
      "source": [
        "def predict(self, x):\n",
        "    # Perform forward propagation and return index of the highest score\n",
        "    o, s = self.forward_propagation(x)\n",
        "    return np.argmax(o, axis=1)\n",
        "\n",
        "RNN.predict = predict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91766709",
      "metadata": {
        "id": "91766709"
      },
      "source": [
        "Let's try and see an example output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5509f471",
      "metadata": {
        "id": "5509f471"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46d62eec",
      "metadata": {
        "id": "46d62eec"
      },
      "outputs": [],
      "source": [
        "print (\"x:\\n%s\\n%s\" % (\" \".join([index_to_word[x] for x in X_train[10]]), X_train[10]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e81127d4",
      "metadata": {
        "id": "e81127d4"
      },
      "outputs": [],
      "source": [
        "np.random.seed(17)\n",
        "model = RNN(vocabulary_size)\n",
        "o, s = model.forward_propagation(X_train[10])\n",
        "print (o.shape, o)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9c26e0",
      "metadata": {
        "id": "ed9c26e0"
      },
      "source": [
        "For each word in the sentence (16 above), our model made 8000 predictions representing probabilities of the next word. \n",
        "\n",
        "That is because we picked a vocabulary size $C = 8000$, so our output layer has $8000$ neurons.\n",
        "\n",
        "The following gives the indices of the highest probability predictions for each word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4809f12d",
      "metadata": {
        "id": "4809f12d"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_train[10])\n",
        "print (predictions.shape, predictions)\n",
        "print (\"x:\\n%s\" % (\" \".join([index_to_word[x] for x in predictions])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "911d390e",
      "metadata": {
        "id": "911d390e"
      },
      "source": [
        "Ok, all parameters are as of yet untrained.\n",
        "\n",
        "## Calculating the Loss\n",
        "\n",
        "To train our network we need a way to measure the errors it makes. We call this the loss function $L$, and our goal is find the parameters $U,V$ and $W$ that minimize the loss function for our training data. \n",
        "\n",
        "A common choice for the loss function is the [cross-entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). \n",
        "\n",
        "If we have $N$ training examples (words in our text) and $C$ classes (the size of our vocabulary) then the loss with respect to our predictions $o$ and the true labels $y$ is given by:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "L(y,o) = - \\frac{1}{N} \\sum_{n \\in N} y_{n} \\log o_{n}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "The formula looks a bit complicated, but all it really does is sum over our training examples and add to the loss based on how off our prediction are. The further away $y$ (the correct words) and $o$ (our predictions), the greater the loss will be. We implement the function `calculate_loss`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69e0369c",
      "metadata": {
        "id": "69e0369c"
      },
      "outputs": [],
      "source": [
        "def calculate_total_loss(self, x, y):\n",
        "    L = 0\n",
        "    # For each sentence...\n",
        "    for i in np.arange(len(y)):\n",
        "        o, s = self.forward_propagation(x[i])\n",
        "        # We only care about our prediction of the \"correct\" words\n",
        "        correct_word_predictions = o[np.arange(len(y[i])), y[i]]\n",
        "        # Add to the loss based on how off we were\n",
        "        L += -1 * np.sum(np.log(correct_word_predictions))\n",
        "    return L\n",
        "\n",
        "def calculate_loss(self, x, y):\n",
        "    # Divide the total loss by the number of training examples\n",
        "    N = np.sum((len(y_i) for y_i in y))\n",
        "    return self.calculate_total_loss(x,y)/N\n",
        "\n",
        "RNN.calculate_total_loss = calculate_total_loss\n",
        "RNN.calculate_loss = calculate_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0259513",
      "metadata": {
        "id": "d0259513"
      },
      "source": [
        "What should the loss be should be for random predictions? \n",
        "\n",
        "That will give us a baseline and make sure our implementation is correct. \n",
        "\n",
        "We have $C$ words in our vocabulary, so each word should be (on average) predicted with probability $1/C$, which would yield a loss of $L = -\\frac{1}{N} N \\log\\frac{1}{C} = \\log C$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c5e5760",
      "metadata": {
        "id": "9c5e5760"
      },
      "outputs": [],
      "source": [
        "# Limit to 1000 examples to save time\n",
        "print (\"Expected Loss for random predictions: %f\" % np.log(vocabulary_size))\n",
        "print (\"Actual loss: %f\" % model.calculate_loss(X_train, y_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea71ba9b",
      "metadata": {
        "id": "ea71ba9b"
      },
      "source": [
        "## Training with Backpropagation Through Time\n",
        "\n",
        "We iterate over all our training examples and during each iteration we nudge the parameters into a direction that reduces the error. \n",
        "\n",
        "These directions are given by the gradients on the loss: $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$. \n",
        "\n",
        "We also need a *learning rate*, which defines how big of a step we want to make in each iteration. \n",
        "\n",
        "Because the layer weight parameters are shared by all time steps in the network, the gradient at each output depends not only on the calculations of the current time step, but also the previous time steps!\n",
        "\n",
        "We take as input a training example $(x,y)$ and return the gradients $\\frac{\\partial L}{\\partial U}, \\frac{\\partial L}{\\partial V}, \\frac{\\partial L}{\\partial W}$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a58f7269",
      "metadata": {
        "id": "a58f7269"
      },
      "outputs": [],
      "source": [
        "def bptt(self, x, y):\n",
        "    T = len(y)\n",
        "    \n",
        "    # Perform forward propagation\n",
        "    o, s = self.forward_propagation(x)\n",
        "    \n",
        "    # We accumulate the gradients in these variables\n",
        "    dLdU = np.zeros(self.U.shape)\n",
        "    dLdV = np.zeros(self.V.shape)\n",
        "    dLdW = np.zeros(self.W.shape)\n",
        "    delta_o = o\n",
        "    delta_o[np.arange(len(y)), y] -= 1.\n",
        "    \n",
        "    # For each output backwards...\n",
        "    for t in np.arange(T)[::-1]:\n",
        "        dLdV += np.outer(delta_o[t], s[t].T)\n",
        "        \n",
        "        # Initial delta calculation\n",
        "        delta_t = self.V.T.dot(delta_o[t]) * (1 - (s[t] ** 2))\n",
        "        \n",
        "        # Backpropagation through time (for at most self.bptt_truncate steps)\n",
        "        for bptt_step in np.arange(max(0, t-self.bptt_truncate), t+1)[::-1]:\n",
        "            \n",
        "            # print \"Backpropagation step t=%d bptt step=%d \" % (t, bptt_step)\n",
        "            dLdW += np.outer(delta_t, s[bptt_step-1])              \n",
        "            dLdU[:,x[bptt_step]] += delta_t\n",
        "            \n",
        "            # Update delta for next step\n",
        "            delta_t = self.W.T.dot(delta_t) * (1 - s[bptt_step-1] ** 2)\n",
        "            \n",
        "    return [dLdU, dLdV, dLdW]\n",
        "\n",
        "RNN.bptt = bptt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1c3545",
      "metadata": {
        "id": "4b1c3545"
      },
      "source": [
        "## Gradient Checking\n",
        "\n",
        "Whenever you implement backpropagation it is good idea to also implement *gradient checking*, which is a way of verifying that your implementation is correct. The idea behind gradient checking is that derivative of a parameter is equal to the slope at the point, which we can approximate by slightly changing the parameter and then dividing by the change:\n",
        "\n",
        "$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial L}{\\partial \\theta} \\approx \\lim_{h \\to 0} \\frac{J(\\theta + h) - J(\\theta -h)}{2h}\n",
        "\\end{aligned}\n",
        "$\n",
        "\n",
        "We then compare the gradient we calculated using backpropagation to the gradient we estimated with the method above. If there's no large difference we are good. The approximation needs to calculate the total loss for *every* parameter, so that gradient checking is very expensive (remember, we had more than a million parameters in the example above). So it's a good idea to perform it on a model with a smaller vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad83adb",
      "metadata": {
        "id": "0ad83adb"
      },
      "outputs": [],
      "source": [
        "def gradient_check(self, x, y, h=0.001, error_threshold=0.01):\n",
        "    \n",
        "    # Calculate the gradients using backpropagation. We want to checker if these are correct.\n",
        "    bptt_gradients = model.bptt(x, y)\n",
        "    \n",
        "    # List of all parameters we want to check.\n",
        "    model_parameters = ['U', 'V', 'W']\n",
        "    \n",
        "    # Gradient check for each parameter\n",
        "    for pidx, pname in enumerate(model_parameters):\n",
        "        \n",
        "        # Get the actual parameter value from the mode, e.g. model.W\n",
        "        parameter = operator.attrgetter(pname)(self)\n",
        "        print(\"Performing gradient check for parameter %s with size %d.\" % (pname, np.prod(parameter.shape)))\n",
        "               \n",
        "        # Iterate over each element of the parameter matrix, e.g. (0,0), (0,1), ...\n",
        "        it = np.nditer(parameter, flags=['multi_index'], op_flags=['readwrite'])\n",
        "        while not it.finished:\n",
        "            ix = it.multi_index\n",
        "               \n",
        "            # Save the original value so we can reset it later\n",
        "            original_value = parameter[ix]\n",
        "               \n",
        "            # Estimate the gradient using (f(x+h) - f(x-h))/(2*h)\n",
        "            parameter[ix] = original_value + h\n",
        "            gradplus = model.calculate_total_loss([x],[y])\n",
        "            parameter[ix] = original_value - h\n",
        "            gradminus = model.calculate_total_loss([x],[y])\n",
        "            estimated_gradient = (gradplus - gradminus)/(2*h)\n",
        "               \n",
        "            # Reset parameter to original value\n",
        "            parameter[ix] = original_value\n",
        "               \n",
        "            # The gradient for this parameter calculated using backpropagation\n",
        "            backprop_gradient = bptt_gradients[pidx][ix]\n",
        "               \n",
        "            # calculate The relative error: (|x - y|/(|x| + |y|))\n",
        "            relative_error = np.abs(backprop_gradient - estimated_gradient) / (\n",
        "                                np.abs(backprop_gradient) + np.abs(estimated_gradient))\n",
        "            \n",
        "               # If the error is to large fail the gradient check\n",
        "            if relative_error > error_threshold:\n",
        "                print( \"Gradient Check ERROR: parameter=%s ix=%s\" % (pname, ix))\n",
        "                print( \"+h Loss: %f\" % gradplus)\n",
        "                print( \"-h Loss: %f\" % gradminus)\n",
        "                print( \"Estimated_gradient: %f\" % estimated_gradient)\n",
        "                print( \"Backpropagation gradient: %f\" % backprop_gradient)\n",
        "                print( \"Relative Error: %f\" % relative_error)\n",
        "                return \n",
        "            it.iternext()\n",
        "               \n",
        "        print( \"Gradient check for parameter %s passed.\" % (pname))\n",
        "\n",
        "RNN.gradient_check = gradient_check"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abe6ce42",
      "metadata": {
        "id": "abe6ce42"
      },
      "source": [
        "## Gradient Descent Implementation\n",
        "\n",
        "A function `sdg_step` calculates the gradients and performs the updates for one batch. \n",
        "\n",
        "Then, an outer loop that iterates through the training set and adjusts the learning rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ac757bb",
      "metadata": {
        "id": "9ac757bb"
      },
      "outputs": [],
      "source": [
        "# Performs one step of SGD.\n",
        "def numpy_sdg_step(self, x, y, learning_rate):\n",
        "    # Calculate the gradients\n",
        "    dLdU, dLdV, dLdW = self.bptt(x, y)\n",
        "    \n",
        "    # Change parameters according to gradients and learning rate\n",
        "    self.U -= learning_rate * dLdU\n",
        "    self.V -= learning_rate * dLdV\n",
        "    self.W -= learning_rate * dLdW\n",
        "\n",
        "RNN.sgd_step = numpy_sdg_step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8ad5aab",
      "metadata": {
        "id": "f8ad5aab"
      },
      "outputs": [],
      "source": [
        "# Outer SGD Loop\n",
        "# - model: The RNN model instance\n",
        "# - X_train: The training data set\n",
        "# - y_train: The training data labels\n",
        "# - learning_rate: Initial learning rate for SGD\n",
        "# - nepoch: Number of times to iterate through the complete dataset\n",
        "# - evaluate_loss_after: Evaluate the loss after this many epochs\n",
        "\n",
        "def train_with_sgd(model, X_train, y_train, learning_rate=0.005, nepoch=100, evaluate_loss_after=5):\n",
        "    # We keep track of the losses so we can plot them later\n",
        "    losses = []\n",
        "    num_examples_seen = 0\n",
        "    \n",
        "    for epoch in range(nepoch):\n",
        "        \n",
        "        # Optionally evaluate the loss\n",
        "        if (epoch % evaluate_loss_after == 0):\n",
        "            loss = model.calculate_loss(X_train, y_train)\n",
        "            losses.append((num_examples_seen, loss))\n",
        "            time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            print (\"%s: Loss after num_examples_seen=%d epoch=%d: %f\" % (time, num_examples_seen, epoch, loss))\n",
        "            \n",
        "            # Adjust the learning rate if loss increases\n",
        "            if (len(losses) > 1 and losses[-1][1] > losses[-2][1]):\n",
        "                learning_rate = learning_rate * 0.5  \n",
        "                print (\"Setting learning rate to %f\" % learning_rate)\n",
        "            sys.stdout.flush()\n",
        "            \n",
        "        # For each training example...\n",
        "        for i in range(len(y_train)):\n",
        "            \n",
        "            # One SGD step\n",
        "            model.sgd_step(X_train[i], y_train[i], learning_rate)\n",
        "            num_examples_seen += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96ed86a9",
      "metadata": {
        "id": "96ed86a9"
      },
      "source": [
        "Let's get a sense of how long it would take to train our network:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c95ea5d",
      "metadata": {
        "id": "4c95ea5d"
      },
      "outputs": [],
      "source": [
        "np.random.seed(21)\n",
        "model = RNN(vocabulary_size)\n",
        "%timeit model.sgd_step(X_train[20], y_train[20], 0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "488c6c1e",
      "metadata": {
        "id": "488c6c1e"
      },
      "outputs": [],
      "source": [
        "np.random.seed(10)\n",
        "\n",
        "# Train on a small subset of the data to see what happens\n",
        "model = RNN(vocabulary_size)\n",
        "losses = train_with_sgd(model, X_train, y_train, nepoch=10, evaluate_loss_after=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e605272",
      "metadata": {
        "id": "4e605272"
      },
      "source": [
        "\n",
        "\n",
        "## Generating Text\n",
        "\n",
        "Now that we have a model, albeit not a very trained one, we can ask it to generate new text for us! Let's implement a helper function to generate new sentences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c601f4",
      "metadata": {
        "id": "58c601f4"
      },
      "outputs": [],
      "source": [
        "def generate_sentence(model, senten_max_length):\n",
        "    # We start the sentence with the start token\n",
        "    new_sentence = [word_to_index[sentence_start_token]]\n",
        "    \n",
        "    # Repeat until we get an end token and keep our sentences to less than senten_max_length words for now\n",
        "    while (not new_sentence[-1] == word_to_index[sentence_end_token]) and len(new_sentence) < senten_max_length:\n",
        "        next_word_probs = model.forward_propagation(new_sentence)\n",
        "        sampled_word = word_to_index[unknown_token]\n",
        "        \n",
        "        # We don't want to sample unknown words\n",
        "        while sampled_word == word_to_index[unknown_token]:\n",
        "\n",
        "            samples = np.random.multinomial(1, next_word_probs[0][-1])\n",
        "            sampled_word = np.argmax(samples)\n",
        "            \n",
        "        new_sentence.append(sampled_word)    \n",
        "        sentence_str = [index_to_word[x] for x in new_sentence[1:-1]]\n",
        "    return sentence_str"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d50a7045",
      "metadata": {
        "id": "d50a7045"
      },
      "outputs": [],
      "source": [
        "num_sentences = 10\n",
        "senten_min_length = 7\n",
        "senten_max_length = 20\n",
        "\n",
        "for i in range(num_sentences):\n",
        "    sent = []\n",
        "    # We want long sentences, not sentences with one or two words\n",
        "    while len(sent) < senten_min_length:\n",
        "        sent = generate_sentence(model, senten_max_length)\n",
        "    print (\" \".join(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c201db",
      "metadata": {
        "id": "22c201db"
      },
      "source": [
        "# Question 2 RNN with tenserflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bc670e5",
      "metadata": {
        "id": "2bc670e5"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "#from keras.models import Sequential\n",
        "#from keras import layers\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "from six.moves import range"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adef9561",
      "metadata": {
        "id": "adef9561"
      },
      "source": [
        "# A one-hot encoding python class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e6cd3b3",
      "metadata": {
        "id": "8e6cd3b3"
      },
      "outputs": [],
      "source": [
        "class CharacterTable(object):\n",
        "    \"\"\"Given a set of characters:\n",
        "    + Encode them to a one-hot integer representation\n",
        "    + Decode the one-hot or integer representation to their character output\n",
        "    + Decode a vector of probabilities to their character output\n",
        "    \"\"\"\n",
        "    def __init__(self, chars):\n",
        "        \"\"\"Initialize character table.\n",
        "\n",
        "        # Arguments\n",
        "            chars: Characters that can appear in the input.\n",
        "        \"\"\"\n",
        "        self.chars = sorted(set(chars))\n",
        "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
        "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
        "\n",
        "    def encode(self, C, num_rows):\n",
        "        \"\"\"One-hot encode given string C.\n",
        "\n",
        "        # Arguments\n",
        "            C: string, to be encoded.\n",
        "            num_rows: Number of rows in the returned one-hot encoding. This is\n",
        "                used to keep the # of rows for each data the same.\n",
        "        \"\"\"\n",
        "        x = np.zeros((num_rows, len(self.chars)))\n",
        "        for i, c in enumerate(C):\n",
        "            x[i, self.char_indices[c]] = 1\n",
        "        return x\n",
        "\n",
        "    def decode(self, x, calc_argmax=True):\n",
        "        \"\"\"Decode the given vector or 2D array to their character output.\n",
        "\n",
        "        # Arguments\n",
        "            x: A vector or a 2D array of probabilities or one-hot representations;\n",
        "                or a vector of character indices (used with `calc_argmax=False`).\n",
        "            calc_argmax: Whether to find the character index with maximum\n",
        "                probability, defaults to `True`.\n",
        "        \"\"\"\n",
        "        if calc_argmax:\n",
        "            x = x.argmax(axis=-1)\n",
        "        return ''.join(self.indices_char[x] for x in x)\n",
        "\n",
        "\n",
        "class colors:\n",
        "    ok = '\\033[92m'\n",
        "    fail = '\\033[91m'\n",
        "    close = '\\033[0m'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0c6e7a6",
      "metadata": {
        "id": "e0c6e7a6"
      },
      "source": [
        "# Generating training data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb050b6d",
      "metadata": {
        "id": "eb050b6d"
      },
      "outputs": [],
      "source": [
        "''.join(np.random.choice(list('0123456789')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb63e7f2",
      "metadata": {
        "id": "eb63e7f2"
      },
      "outputs": [],
      "source": [
        "int(''.join(np.random.choice(list('0123456789')) for i in range(np.random.randint(1, 4))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "733ce78e",
      "metadata": {
        "id": "733ce78e"
      },
      "outputs": [],
      "source": [
        "np.random.choice(list('0123456789'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0fbaa9d",
      "metadata": {
        "id": "d0fbaa9d"
      },
      "outputs": [],
      "source": [
        "# Parameters for the model and dataset.\n",
        "TRAINING_SIZE = 50000\n",
        "DIGITS = 3\n",
        "REVERSE = False\n",
        "\n",
        "# Maximum length of input is 'int + int' (e.g., '34+78  '). Maximum length of\n",
        "# int is DIGITS.\n",
        "MAXLEN = DIGITS + 1 + DIGITS\n",
        "\n",
        "# All the numbers, plus sign and space for padding.\n",
        "chars = '0123456789+ '\n",
        "ctable = CharacterTable(chars)\n",
        "\n",
        "questions = []\n",
        "expected = []\n",
        "seen = set()\n",
        "print('Generating data...')\n",
        "while len(questions) < TRAINING_SIZE:\n",
        "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
        "                    for i in range(np.random.randint(1, DIGITS + 1))))\n",
        "    a, b = f(), f()\n",
        "    \n",
        "    # Skip any addition questions we've already seen\n",
        "    # Also skip any such that x+Y == Y+x (hence the sorting).\n",
        "    key = tuple(sorted((a, b)))\n",
        "    if key in seen:\n",
        "        continue\n",
        "    seen.add(key)\n",
        "    \n",
        "    # Pad the data with spaces such that it is always MAXLEN.\n",
        "    q = '{}+{}'.format(a, b)\n",
        "    query = q + ' ' * (MAXLEN - len(q))\n",
        "    ans = str(a + b)\n",
        "    \n",
        "    # Answers can be of maximum size DIGITS + 1.\n",
        "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
        "    if REVERSE:\n",
        "        # Reverse the query, e.g., '12+345  ' becomes '  543+21'. (Note the\n",
        "        # space used for padding.)\n",
        "        query = query[::-1]\n",
        "    \n",
        "    # store\n",
        "    questions.append(query)\n",
        "    expected.append(ans)\n",
        "    print(query, ans)\n",
        "    \n",
        "print('Total addition questions:', len(questions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62b4b1da",
      "metadata": {
        "id": "62b4b1da"
      },
      "source": [
        "Let's list our **input** sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491f41e6",
      "metadata": {
        "id": "491f41e6"
      },
      "outputs": [],
      "source": [
        "questions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b37d62",
      "metadata": {
        "id": "12b37d62"
      },
      "source": [
        "Let's list our **output** sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce2db201",
      "metadata": {
        "scrolled": true,
        "id": "ce2db201"
      },
      "outputs": [],
      "source": [
        "expected"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7acc26d",
      "metadata": {
        "id": "c7acc26d"
      },
      "source": [
        "# Training and test data\n",
        "\n",
        "Let's divide all our observations into a *training* set and a *test* set. We will now *encode* our numbers into **one-hot vectors**: \n",
        "\n",
        "The first dimension is the numbers of rows (observations).\n",
        "\n",
        "The second dimension is the number of characters in each sequence.\n",
        "\n",
        "The third dimension is the size of our alphabet of symbols. Here, it's `len('0123456789+ ')`. In our statistical sentence translation notebook, it's the size of the word dictionary (word-to-index mapping).\n",
        "\n",
        "Let's allocate space for the input to the encoder `x`, and the input to the decoder, `y`. They're both 3D **tensors**: The first dimension is the row, and the other two dimensions are the thought-vector representation of the question or the answer.\n",
        "\n",
        "Here's the first question in math format, and in thought-vector format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0af8273f",
      "metadata": {
        "id": "0af8273f"
      },
      "outputs": [],
      "source": [
        "ctable.encode(questions[0], MAXLEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abef35ac",
      "metadata": {
        "id": "abef35ac"
      },
      "source": [
        "And here's the answer, in math format, and in thought-vector format:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4180a0d4",
      "metadata": {
        "id": "4180a0d4"
      },
      "outputs": [],
      "source": [
        "ctable.encode(expected[0], DIGITS + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c19087c",
      "metadata": {
        "id": "7c19087c"
      },
      "source": [
        "Let's create the thought-vectors for our entire training dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04b7a1ff",
      "metadata": {
        "id": "04b7a1ff"
      },
      "outputs": [],
      "source": [
        "print('Vectorization into thought:')\n",
        "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool)\n",
        "for i, sentence in enumerate(questions):\n",
        "    x[i] = ctable.encode(sentence, MAXLEN)\n",
        "for i, sentence in enumerate(expected):\n",
        "    y[i] = ctable.encode(sentence, DIGITS + 1)\n",
        "\n",
        "# Shuffle (x, y) in unison as the later parts of x will almost all be larger digits.\n",
        "indices = np.arange(len(y))\n",
        "np.random.shuffle(indices)\n",
        "x = x[indices]\n",
        "y = y[indices]\n",
        "\n",
        "# Explicitly set apart 10% for validation data that we never train over.\n",
        "split_at = len(x) - len(x) // 10\n",
        "(x_train, x_val) = x[:split_at], x[split_at:]\n",
        "(y_train, y_val) = y[:split_at], y[split_at:]\n",
        "\n",
        "print('Training Data:')\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "print()\n",
        "\n",
        "print('Validation Data:')\n",
        "print(x_val.shape)\n",
        "print(y_val.shape)\n",
        "print()\n",
        "\n",
        "print('Example:')\n",
        "print('The first row of input data is encoded internally as:')\n",
        "print(x_train[0])\n",
        "print()\n",
        "print('The first row of output data is encoded internally as:')\n",
        "print(y_train[0])\n",
        "print()\n",
        "print('These internal representations represent these signals:')\n",
        "print(ctable.decode(x_train[0]))\n",
        "print(ctable.decode(y_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97012a1d",
      "metadata": {
        "id": "97012a1d"
      },
      "source": [
        "## RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c0284bc",
      "metadata": {
        "id": "3c0284bc"
      },
      "outputs": [],
      "source": [
        "# Try replacing with GRU, or SimpleRNN.\n",
        "RNN = layers.LSTM\n",
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 128\n",
        "LAYERS = 1\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE.\n",
        "# Note: In a situation where your input sequences have a variable length,\n",
        "# use input_shape=(None, num_feature).\n",
        "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "\n",
        "# As the decoder RNN's input, repeatedly provide with the last output of\n",
        "# RNN for each time step. Repeat 'DIGITS + 1' times as that's the maximum\n",
        "# length of output, e.g., when DIGITS=3, max output is 999+999=1998.\n",
        "model.add(layers.RepeatVector(DIGITS + 1))\n",
        "\n",
        "# The decoder RNN could be multiple layers stacked or a single layer.\n",
        "for _ in range(LAYERS):\n",
        "    # By setting return_sequences to True, return not only the last output but\n",
        "    # all the outputs so far in the form of (num_samples, timesteps,\n",
        "    # output_dim). This is necessary as TimeDistributed in the below expects\n",
        "    # the first dimension to be the timesteps.\n",
        "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
        "\n",
        "# Apply a dense layer to the every temporal slice of an input. For each of step\n",
        "# of the output sequence, decide which character should be chosen.\n",
        "# We require DIGITS + 1 output vectors for our result. We will use the same fully \n",
        "# connected layer (Dense) to output each vector. To use the same layer DIGITS + 1\n",
        "# times, we wrap it in a TimeDistributed() wrapper layer\n",
        "model.add(layers.TimeDistributed(layers.Dense(len(chars), activation='softmax')))\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41686be8",
      "metadata": {
        "id": "41686be8"
      },
      "source": [
        "We had to connect the encoder to the decoder and they *did not originally fit*!\n",
        "\n",
        "With\n",
        "```(python)\n",
        "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars))))\n",
        "```\n",
        "\n",
        "..the encoder takes in a **3D tensor** of 50,000 rows, MAXLEN timesteps, `len(char)` features, and...\n",
        "\n",
        "..for each observation, the encoder will produce a **2D matrix** of 128 rows and `len(chars)` columns (after MAXLEN timesteps), while the decoder needs a **3D tensor**, not just 128 rows and `len(chars)` features, which it will then run over `DIGITS + 1` timesteps. That's a problem!\n",
        "\n",
        "Keras' [RepeatVector](https://keras.io/layers/core/#repeatvector) layer is used like an adapter to fit the encoder and decoder. We configure `RepeatVector` to repeat the input `DIGITS + 1` times. This creates a 3D output comprised of `DIGITS + 1` copies of 128 x `len(char)` features, that we decode `DIGITS + 1` times using the same fully connected layer for each of the `DIGITS + 1` desired output vectors.\n",
        "\n",
        "If you comment out the `RepeatVector` line, you will get:\n",
        "```(python)\n",
        "ValueError: Input 0 is incompatible with layer lstm_xx: expected ndim=3, found ndim=2\n",
        "```\n",
        "Try it out!\n",
        "\n",
        "Finally, the fully connected output layer will use a softmax activation function to output values in the range \\[0,1\\], so some closer to `False`, and others closer to` True`. We will approximate by **rounding off** to 0 or 1 for our final prediction. \n",
        "\n",
        ">**Note**: Why not leverage one output for *each* time step for the input sequence, rather than one output for the *final* timestep of the input sequence (which is what we do here)? The alternate option would give us intermediate timestep information, which we don't leveerage here. This is discussed [here](https://github.com/fchollet/keras/issues/395)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3f68d71",
      "metadata": {
        "id": "e3f68d71"
      },
      "source": [
        "# Training\n",
        "\n",
        "Let's train our RNN for 35 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1645ff65",
      "metadata": {
        "id": "1645ff65"
      },
      "outputs": [],
      "source": [
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for iteration in range(1, 2):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              validation_data=(x_val, y_val))\n",
        "    \n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors with green and red boxes\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        #preds = model.predict_classes(rowx, verbose=0)\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        print(\"debugging:\")\n",
        "        print(type(preds[0]))\n",
        "        print(\"now decoding...:\")\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
        "        print('T', correct, end=' ')\n",
        "        if correct == guess:\n",
        "            print(colors.ok + '☑' + colors.close, end=' ')\n",
        "        else:\n",
        "            print(colors.fail + '☒' + colors.close, end=' ')\n",
        "        print(guess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f50bd9",
      "metadata": {
        "id": "d9f50bd9"
      },
      "outputs": [],
      "source": [
        "# Train the model each generation and show predictions against the validation\n",
        "# dataset.\n",
        "for iteration in range(1, 35):\n",
        "    print()\n",
        "    print('-' * 50)\n",
        "    print('Iteration', iteration)\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              epochs=1,\n",
        "              validation_data=(x_val, y_val))\n",
        "    \n",
        "    # Select 10 samples from the validation set at random so we can visualize\n",
        "    # errors with green and red boxes\n",
        "    for i in range(10):\n",
        "        ind = np.random.randint(0, len(x_val))\n",
        "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
        "        preds = np.argmax(model.predict(rowx), axis=-1)\n",
        "        q = ctable.decode(rowx[0])\n",
        "        correct = ctable.decode(rowy[0])\n",
        "        print(\"debugging:\")\n",
        "        print(type(preds[0]))\n",
        "        print(\"now decoding...:\")\n",
        "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
        "        print('Q', q[::-1] if REVERSE else q, end=' ')\n",
        "        print('T', correct, end=' ')\n",
        "        if correct == guess:\n",
        "            print(colors.ok + '☑' + colors.close, end=' ')\n",
        "        else:\n",
        "            print(colors.fail + '☒' + colors.close, end=' ')\n",
        "        print(guess)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ae9728",
      "metadata": {
        "id": "84ae9728"
      },
      "source": [
        "The loss is pretty low after only 30 epochs (and the samples are all correct), so I stopped the training!\n",
        "\n",
        "Let's evaluate the accuracy of our model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35db661a",
      "metadata": {
        "id": "35db661a"
      },
      "outputs": [],
      "source": [
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(x_val, y_val)\n",
        "print('Accuracy: %.2f' % (accuracy*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "daedf0b4",
      "metadata": {
        "id": "daedf0b4"
      },
      "source": [
        "99%.. Not bad.. We now know how to add numbers!\n",
        "\n",
        "Let's add two numbers and test our network. We need to make sure we encode our numbers as vectors of the same type and shape used for training!\n",
        "\n",
        "Let's pick two numbers at random, sum them up, and pad them to 7 characters long:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "579b7fbb",
      "metadata": {
        "id": "579b7fbb"
      },
      "outputs": [],
      "source": [
        "x = np.random.randint(0, 100)\n",
        "y = np.random.randint(0, 100)\n",
        "z = x + y\n",
        "print(x,y,z)\n",
        "print()\n",
        "\n",
        "x_plus_y_buffered = str(x) + '+' + str(y) + (7 - len(str(x) + '+' + str(y))) * ' '\n",
        "x_plus_y_buffered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6e49fe2",
      "metadata": {
        "id": "f6e49fe2"
      },
      "source": [
        "Let's do the same thing with the result:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c668609a",
      "metadata": {
        "id": "c668609a"
      },
      "outputs": [],
      "source": [
        "z_buffered = str(z) + (4 - len(str(z))) * ' '\n",
        "z_buffered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f800d07a",
      "metadata": {
        "id": "f800d07a"
      },
      "source": [
        "Now let's encode these numbers in the internal representation of the neural net.\n",
        "\n",
        ">**Note**: Every kind of sensory inpur, whether it's from your eyes, your ears, your nose,... get encoded in *internal* brain representations. So, we're doing exactly the same thing here. Except that the internal representations of sensory inputs in our brains happen at a darwinian scale. That goes to show that finding the *right* encoding may make a big difference in the training and have an effect on overall accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dcbc679",
      "metadata": {
        "id": "4dcbc679"
      },
      "outputs": [],
      "source": [
        "x_plus_y_encoded = ctable.encode(x_plus_y_buffered, 7)\n",
        "x_plus_y_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b73fff4",
      "metadata": {
        "id": "7b73fff4"
      },
      "outputs": [],
      "source": [
        "ctable.decode(x_plus_y_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1098b5d",
      "metadata": {
        "id": "e1098b5d"
      },
      "outputs": [],
      "source": [
        "z_encoded = ctable.encode(z_buffered, 4)\n",
        "z_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94314974",
      "metadata": {
        "id": "94314974"
      },
      "outputs": [],
      "source": [
        "ctable.decode(z_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29e29ce9",
      "metadata": {
        "id": "29e29ce9"
      },
      "source": [
        "We need to convert to booleans, since that is how we trained our RNN:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d20454",
      "metadata": {
        "id": "e8d20454"
      },
      "outputs": [],
      "source": [
        "x_plus_y = str(x) + '+' + str(y)\n",
        "x_plus_y_tf = np.zeros((len(x_plus_y_buffered), len(chars)), dtype=np.bool)\n",
        "x_plus_y_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cf37ecb",
      "metadata": {
        "id": "8cf37ecb"
      },
      "outputs": [],
      "source": [
        "list(enumerate(x_plus_y_tf))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb5fa41",
      "metadata": {
        "id": "7cb5fa41"
      },
      "outputs": [],
      "source": [
        "ctable.encode('2', 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5403055f",
      "metadata": {
        "id": "5403055f"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(x_plus_y_tf):\n",
        "    x_plus_y_tf[i] = ctable.encode(x_plus_y_buffered[i], 1)\n",
        "x_plus_y_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b56654f",
      "metadata": {
        "id": "5b56654f"
      },
      "outputs": [],
      "source": [
        "z_tf = np.zeros((len(z_buffered), len(chars)), dtype=np.bool)\n",
        "z_tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c02e03",
      "metadata": {
        "id": "04c02e03"
      },
      "outputs": [],
      "source": [
        "for i, sentence in enumerate(z_tf):\n",
        "    z_tf[i] = ctable.encode(z_buffered[i], 1)\n",
        "z_tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "851043e9",
      "metadata": {
        "id": "851043e9"
      },
      "source": [
        "We're now ready for a **forward step** through our RNN. `Keras`' `predict_classes()` API takes in an *array* of inputs, so we arbitrarily double up our input: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bb7ab41",
      "metadata": {
        "id": "9bb7ab41"
      },
      "outputs": [],
      "source": [
        "x_plus_y_tf2 = np.array((x_plus_y_tf, x_plus_y_tf), dtype=np.bool)\n",
        "x_plus_y_tf2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5284f816",
      "metadata": {
        "id": "5284f816"
      },
      "source": [
        "We're now ready to call `predict_classes()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a397c3",
      "metadata": {
        "id": "42a397c3"
      },
      "outputs": [],
      "source": [
        "preds = np.argmax(model.predict(x_plus_y_tf2), axis=-1)\n",
        "preds"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c4d0c3",
      "metadata": {
        "id": "82c4d0c3"
      },
      "source": [
        "Let's decode that representation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb485a55",
      "metadata": {
        "scrolled": true,
        "id": "cb485a55"
      },
      "outputs": [],
      "source": [
        "ctable.decode(preds[0], calc_argmax=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f0545ea",
      "metadata": {
        "id": "5f0545ea"
      },
      "outputs": [],
      "source": [
        "# evaluate the keras model\n",
        "_, accuracy = model.evaluate(x_val, y_val)\n",
        "print('Accuracy: %.2f' % (accuracy*100))\n",
        "\n",
        "\n",
        "\n",
        "x = np.random.randint(0, 100)\n",
        "y = np.random.randint(0, 100)\n",
        "z = x + y\n",
        "print(x,y,z)\n",
        "print()\n",
        "\n",
        "x_plus_y_buffered = str(x) + '+' + str(y) + (7 - len(str(x) + '+' + str(y))) * ' '\n",
        "x_plus_y_buffered\n",
        "\n",
        "\n",
        "\n",
        "z_buffered = str(z) + (4 - len(str(z))) * ' '\n",
        "z_buffered\n",
        "\n",
        "x_plus_y_encoded = ctable.encode(x_plus_y_buffered, 7)\n",
        "x_plus_y_encoded\n",
        "\n",
        "ctable.decode(x_plus_y_encoded)\n",
        "\n",
        "z_encoded = ctable.encode(z_buffered, 4)\n",
        "z_encoded\n",
        "\n",
        "ctable.decode(z_encoded)\n",
        "\n",
        "\n",
        "\n",
        "x_plus_y = str(x) + '+' + str(y)\n",
        "x_plus_y_tf = np.zeros((len(x_plus_y_buffered), len(chars)), dtype=np.bool)\n",
        "x_plus_y_tf\n",
        "\n",
        "list(enumerate(x_plus_y_tf))\n",
        "\n",
        "ctable.encode('2', 1)\n",
        "\n",
        "for i, sentence in enumerate(x_plus_y_tf):\n",
        "    x_plus_y_tf[i] = ctable.encode(x_plus_y_buffered[i], 1)\n",
        "x_plus_y_tf\n",
        "\n",
        "z_tf = np.zeros((len(z_buffered), len(chars)), dtype=np.bool)\n",
        "z_tf\n",
        "\n",
        "for i, sentence in enumerate(z_tf):\n",
        "    z_tf[i] = ctable.encode(z_buffered[i], 1)\n",
        "z_tf\n",
        "\n",
        "\n",
        "x_plus_y_tf2 = np.array((x_plus_y_tf, x_plus_y_tf), dtype=np.bool)\n",
        "x_plus_y_tf2\n",
        "\n",
        "\n",
        "\n",
        "preds = np.argmax(model.predict(x_plus_y_tf2), axis=-1)\n",
        "preds\n",
        "\n",
        "\n",
        "\n",
        "ctable.decode(preds[0], calc_argmax=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training a Dense network to add 2 numbers"
      ],
      "metadata": {
        "id": "u9PifSnXXTTw"
      },
      "id": "u9PifSnXXTTw"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(4, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "7sU1jHr0XKJ4"
      },
      "id": "7sU1jHr0XKJ4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Generate random training data\n",
        "X_train = np.random.randint(0, 100, size=(10000, 2))\n",
        "y_train = X_train[:, 0] + X_train[:, 1]\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n"
      ],
      "metadata": {
        "id": "mMU1mx90XMwX"
      },
      "id": "mMU1mx90XMwX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict the sum of two numbers\n",
        "X_test = np.array([[142, 118], [234, 99], [173, 27]])\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Print the predictions\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "w2lmHZMpXdz7"
      },
      "id": "w2lmHZMpXdz7",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}